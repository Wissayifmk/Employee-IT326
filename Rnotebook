---
title: "Arab Region Employees"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

### 1.0 Problem:

There is a lack of data regarding the factors that influence the
salaries of Arab employees. This knowledge gap makes it difficult for
organizations and authorities to make well-informed choices about future
employee planning, and providing fair salaries across the Arab world. To
address this issue, we aim to use the Arab Employee dataset, which will
provide important data on Arab professionals' qualifications, locations,
and years of experience. We can help organizations and authorities make
better decisions by getting this data, thereby encouraging fair salaries
practices and enhancing people management strategies in the Arab area.

### 2.0 Data Mining Tasks:

Data mining is essential for predicting the likelihood of achieving fair
salary classifications and employing clustering techniques. By utilizing
data mining algorithms on a vast dataset that encompasses the
qualifications, geographic locations, and years of experience of diverse
professionals, we can extract valuable insights.

#### Here are three specific goals we have for this dataset:

#### 2.1 Classification Goal:

We want to group employees into different categories based on their
qualifications, country location, and experience. By doing this, we can
see patterns and trends among different groups. This will help us
understand if there are differences in pay based on qualifications and
experience, and it will help us identify talented individuals. With this
information, we can develop strategies to support career growth and make
sure people are being paid fairly.

#### 2.2 Clustering Goal:

We want to group similar data points so we grouped employees with
similar salaries together. By doing this, will help in uncovering hidden
patterns and structures within the data, making it easier to understand
the inherent relationships between different employees' info. It also
makes it feasible to divide our enormous dataset into smaller,
easier-to-understand subsets. This can make analyzing complicated
datasets easier.

#### 2.3 Defect Prediction Goal:

We want to use machine learning and statistical techniques to create
models that can predict any issues or anomalies with employee salaries.
This could include things like unfair pay practices, wage gaps, or
differences based on qualifications or experience. By catching these
problems early, we can fix them and make sure everyone is being treated
fairly. This analysis will help us improve compensation policies and
make better decisions about managing talent.

#### By achieving these goals, we can:

Understand how salaries are distributed among different employee groups.
Identify any biases or unfairness in pay based on qualifications,
experience, or location. Develop strategies for hiring, developing, and
keeping talented employees. Make sure pay practices are fair and
transparent. Catch and fix salary-related issues before they become
bigger problems. Plan for the future workforce and allocate resources
effectively. Make better decisions about pay and talent management in
the Arab region. Overall, collecting the Arab Employee dataset with
information about salary, qualifications, location, and experience is
really important. It will help us analyze the data and make fair
decisions that create a good working environment for Arab professionals.

### 3.0 Data 

### 3.1 Source of the dataset:

Kaggle

### 3.2 link of the dataset:

<https://www.kaggle.com/datasets/qusaybtoush1990/employes> ^[1]^

### 3.3 General Information about Employees dataset:

Our dataset contains 10 attributes, and 740 objects (See Table 1).
Furthermore, the class label for our dataset is Salary.

##### Table 1: General Information about the attributes for Employees dataset.

| Attributes name | Description                       | Data type | Possible values                      |
|------------------|------------------|------------------|-------------------|
| ID              | Employee id                       | Nominal   | 6 values                             |
| Education       | Education level for the employee  | Nominal   | Prof-Doctor-Bachelor\\...            |
| Department      | Department that employee works in | Nominal   | FSL-NEI-IT\\...                      |
| Job status      | Employment situatio               | Nominal   | Full time-part time-cotract          |
| Location        | The location of the job           | Nominal   | saudi arabia-United Arab Emirates... |
| Start date      | Date to start the job             | Nominal   | 7Jan11-1Jan20                        |
| Years           | Years of work \| Numeric          | Numeric   | 0-9                                  |
| Salary          | Salary of the employee            | Numeric   | 650-25000                            |
| Job rate        | Employee evaluation               | Numeric   | 1-13                                 |
| Permission      | The access rights and privileges  | Numeric   | 1-14                                 |

#### 3.4 Sample of Employees dataset

```{r}
View(Employees)
```

#### 3.5 Number of column and rows

```{r}
dim(Employees)
```

#### 3.6 statiscal summarise -Nominal-

```{r}
summary(Employees$Education)
```

```{r}
summary(Employees$Department)
```

```{r}
summary(Employees$'Job Status')
```

```{r}
summary(Employees$Location)
```

#### 3.7 statiscal summarise -Numerical-

```{r}
summary(Employees$Years)
```

```{r}
summary(Employees$Salary)
```

```{r}
summary(Employees$'Job Rate')
```

```{r}
summary(Employees$Permissions)
```

Description of summary: This is the summary of all data set including
Nominal and Numerical Data.

#### 3.8 Code for variance

```{r}
var(Employees$Salary)
```

### 3.9 Graphs

As we know, graphs are great tools for analyzing patterns and
relationships in data because they provide a visual way to explore and
evaluate data. So, in this section, we discussed some plots to have a
full review of our data.

#### 3.9.1 Box plot for salary and years

```{r}
boxplot(Employees$Salary)
```

Description for Salary boxplot: The Salary boxplot illustrates that the
salaries are evenly distributed around the middle point, indicating a
relatively balanced salary distribution.

```{r}
boxplot(Employees$Years)
```

Description for Years boxplot: The Years of Work boxplot illustrates how
the values in the dataset have relatively balanced around the median
value of 4.

```{r}
boxplot(Employees$'Job Rate')
```

Description for Job rate boxplot: The Job rate boxplot is relatively
balanced around the median value of 5. In a more comprehensive analysis,
there is a spread in the job rates above 50% of the dataset.

```{r}
boxplot(Employees$Permissions)
```

Description for Permissions boxplot: The Permissions boxplot reveals
several important insights about the distribution of Permissions. It
illustrates that the values of the attribute are almost balanced near to
the median.

#### 3.9.2 Histogram for permission

```{r}
Permissions <- Employees$Permissions
hist(Permissions)
```

Description for Permissions histogram.: The frequency of Permissions for
the employees in the dataset is represented by the histogram. After
observation, we noticed that the most values lie in approximately from 1
permission to 2. As for the rest of the employees, their Permissions
range from 3 to 14 permission.

#### 3.9.3 Bar Chart for Salary

```{r}
#Bar Chart
Employees$Salary %>% table() %>% barplot()
```

Description for Bar Chart for Salary: The frequency of Salary for the
employees in the dataset is represented by the Bar chart. Since the bar
chart is made for salary, the frequency of each value mostly appears
ones. For best representation; it will be better to do discretisation
before.

#### 3.9.4 pie chart for Education

```{r}
tab <- Employees$Education %>% table()
precentages <- tab %>% prop.table() %>% round(3) * 100 
txt <- paste0(names(tab), '\n', precentages, '%') 
pie(tab, labels=txt)
```

Description for Education pie chart: The pie chart shows that
approximately half of the employees have academic education, and a few
employees are prof, which helps us predict their salaries according to
their level of education.

### 4.0.0 Data preprocessing

In out data, we worked on various Data preprocessing techniques, such
as:Data Cleaning, Data Discretization, Encodings, Feature Selection and
Imbalance the Deta.

##### Snapshot before applying Data preproccessing techniques:

```{r}
dim(Employees)
```

```{r}
str(Employees)
```

### 4.1.0 Data cleaning

#### 4.1.1 To find the total null values in the dataset

```{r}
dim(Employees)
sum(is.na(Employees))
```

#### 4.1.2 Replace missing Data with average value

```{r}
Employees$Years = ifelse(is.na(Employees$Years), ave(Employees$Years, FUN =function(x) mean(x,na.rm=TRUE)), Employees$Years)
View(Employees)
```

Description of data cleaning: We checked if our data contains missing or
null values and replace these missing values with average value to get
more efficient dataset.

#### 4.1.3 Detecting outliers

```{r}
OutSalary = outlier(Employees$Salary, logical =TRUE)
sum(OutSalary)
Find_outlierSalary = which(OutSalary ==TRUE, arr.ind = TRUE)
OutSalary
Find_outlierSalary
```

```{r}
OutPermissions = outlier(Employees$Permissions, logical =TRUE)
sum(OutPermissions)
Find_outlierPre = which(OutPermissions ==TRUE, arr.ind = TRUE)
OutPermissions
Find_outlierPre
```

```{r}
OutJobRate= outlier(Employees$'Job Rate', logical =TRUE)
sum(OutJobRate)
Find_outlierJob = which(OutJobRate ==TRUE, arr.ind = TRUE)
OutJobRate
Find_outlierJob
```

```{r}
OutYears= outlier(Employees$Years, logical =TRUE)
sum(OutYears)
Find_outlierYears = which(OutYears ==TRUE, arr.ind = TRUE)
OutYears
Find_outlierYears
```

```{r}
summary(Employees)
str(Employees)
```

#### 4.1.4 Removing outliers

```{r}
Employees= Employees[-Find_outlierSalary,]
Employees= Employees[-Find_outlierPre,]
Employees= Employees[-Find_outlierJob,]
Employees= Employees[-Find_outlierYears,]
```

Description of detecting outliers, Removing outliers: This function is
used to find outliers-if found- in order to remove them to have better
prediction.

```{r}
summary(Employees)
str(Employees)
```

### 4.2.0 Data Discretization

```{r}
Employees$Salary <- cut(Employees$Salary, 
                        breaks = c( 600 , 1000 , 1600 , 2600 ),
                        labels = c( "600-1000", "1000-1600", "1600-2600"))
```

Description of the discretization: This function arranges the values in
intervals and give every interval a suitable label. by discretization,
the Salary will be simpler to classify or perform other methods.

### 4.3.0 Encoding

```{r}
Employees$Education =  as.numeric(factor(Employees$Education))
```

```{r}
Employees$Department =  as.numeric(factor(Employees$Department))
```

```{r}
Employees$'Job Status' = as.numeric(factor(Employees$'Job Status'))
```

```{r}
Employees$Location = as.numeric(factor(Employees$Location)) 
```

Description of encoding: Encoding is the process of converting
categorical data to numerical format. We need to convert the data to be
able to do clustering.

### 4.4.0 Feature Selection

```{r}
set.seed(123)
Employees$Salary <- as.factor(Employees$Salary )
predictors <- Employees[, -8] 
class_label <- Employees$Salary 
model <- randomForest(predictors, class_label, importance = TRUE)
importance <- importance(model)
ranked_features <- sort(importance[, "MeanDecreaseGini"], decreasing = TRUE)

# Print the ranked features
print(ranked_features)

barplot(ranked_features, horiz = TRUE, col = c("lightblue2"), las = 1, main = "Airline satisfaction Variable Import")
```

Description of feature selection: Feature selection is a process in
machine learning, it is used to improve model performance. In several
datasets, there are many features, and not all of the features
participate equally to the performance of the model. Some of the
features could even make redundancy or some noise. Feature selection
assist in showing the most relevant features, leading to simpler and
more interpretable models. In our dataset the most relevant features
were "Job Status", "Job Rate" and "ID". ^[2]^

### 4.5.0 Data Imbalance

Description of Imbalance: Imbalance refers to a situation where the
distribution of attributes in a dataset is not uniform. without
imbalance tasks such as classification can be effected.

#### 4.5.1 Bar Chart for Salary

```{r}
#Bar Chart
Employees$Salary %>% table() %>% barplot()
```

Description of bar chart after discretization: After doing
discretisation and observation, we noticed that the most values lie in
approximately from 1600 to 2600. while from 600-1000 and 1000-1600 are
close to each other. The range of Salaries has big difference, which we
have to consider it for the classification and clustering.

#### 4.5.2 Convert Salary column to factor

```{r}
Employees$Salary <- as.factor(Employees$Salary)
```

#### 4.5.3 upscaling the data

```{r}
Employees <- upSample(Employees[,-8], Employees$Salary, yname="Salary")
Employees$Salary <- as.numeric(Employees$Salary)
Employees$Salary <- as.factor(Employees$Salary)
```

#### 4.5.4 checking the Salaries observations

```{r}
prop.table(table(Employees$Salary))
plot(Employees$Salary)
title(main = "Data after oversampling", xlab = "Salary", ylab = "observations")
```

### 4.6.0 Data Normalization

Since we have a small range in our dataset, we don't have to use the
data normalization. As for the salary which has a large range; instead,
we used data discretization that will help us in classification.

### 4.6.0 Data Factorisation

We applied it in classification section -5.0.0- which is important for
applying classification technique. But we used it in a training set and
testing set rather than the original which will affect the clustering
technique.

##### Snapshot After applying Data preproccessing techniques:

```{r}
dim(Employees)
```

```{r}
str(Employees)
```

### 5.0.0 Data Mining Techniques

We employed both supervised and unsupervised learning methods on our
dataset, utilizing techniques for classifying and grouping the data.

### 5.1.0 Classification

We used classification techniques to apply supervised learning to our
data. We applied a decision tree for classification, which is a
recursive method that generates a tree with leaf nodes pointing to the
final decisions. Our model will predict the salary classification
(600-1000, 1000-1600, 1600-2600). The rest of the attributes (education,
job rate, job status, location, Years, Department and permission) are
used to make the prediction. As for the ID, since it useless for
information gain we did not use it. Also, to consume the complexity; we
did not use the Start Date attribute. However, to be consistent we used
the same formula for all methods except Gain ratio; since the decision
tree will be complex, so we be content with Job Status, Job Rate ,
Department and Permissions which are the most 4 important attributes as
mentioned in the feature selection.

The dataset is divided into two sets using this technique: The training
set and the testing test. We need training data to train the model, so
to achieve the greatest accuracy, we tried three different sizes of
training subsets: 65%, 70%, and 75%, because the size of our dataset is
limited. Because our model's capacity to accurately forecast class label
for new tuples is dependent on the operation of creating and training
the model, we always assigned the training subset the largest percentage
of our dataset.

```{r}
str(Employees)
```

#### 5.1.1 information gain Information gain

(rpart) is a measure used in decision tree algorithms. It is used to
evaluate the quality of a split in a decision tree by measuring the
reduction in entropy in the dataset. ^[3]^

The information gain approach in rpart considers various potential
splits based on different features in the dataset. For each split, the
information gain is calculated by comparing the entropy of the original
dataset with the weighted average of the entropies of the resulting
subsets. The split that maximizes the information gain is chosen as the
best split, as it leads to subsets with the highest level of homogeneity
or purity.^[3]^

When building a decision tree using rpart, the algorithm iterates
through all possible splits and selects the one with the highest
information gain. This process is repeated recursively for each
resulting subset until a stopping criterion is met, such as reaching a
maximum tree depth or a minimum number of samples in a leaf node.

#### 5.1.2 Gini Index

Gini index (rpart) can be considered as a measure for the level of
impurity in a dataset. When building the tree, decision tree algorithms
frequently use it to determine the split's quality.^[4]^

The approach takes into consideration several possible splits depending
on distinct aspects in the dataset in order to employ the Gini index in
a decision tree. Every split's Gini index is computed, and the split
with the lowest value is chosen. The selected split produces the
resulting subsets with the best level of homogeneity or purity.^[4]^

#### 5.1.3 Gain Ratio

Gain ratio (C.50) is a measure utilized in decision tree algorithms to
assess the quality of a split by considering both the information gain
and the intrinsic information of a feature. It factors in the entropy or
impurity of a dataset and the potential information obtained from
dividing the data based on a specific feature.^[5]^

The algorithm examines the gain ratios of different features while
employing the gain ratio under a decision tree, selecting the feature
with the highest ratio as the ideal split. This will reduce the
likelihood of bias toward attributes with a high number of values or
categories.^[5]^

### 6.0.0 Evaluation and comparison

#### 6.1.0 Classification

#### 6.1.1information gain

#### 6.1.2 information gain (70% , 30%)

```{r}
# 1.a fixed random seed to make results reproducible
set.seed(1234)
```

```{r}
# 2.Split the datasets into two subsets: Training(70%) and Testing(30%):
ind <- sample(2, nrow(Employees), replace = TRUE, prob = c(0.7, 0.3))
train.data <- Employees[ind == 1, ]
test.data <- Employees[ind == 2, ]
```

```{r}
# 3.Converting attribute to factor
train.data$Years <- factor(train.data$Years)
test.data$Years <- factor(test.data$Years)
train.data$'Job Rate' <- factor(train.data$'Job Rate')
test.data$'Job Rate' <- factor(test.data$'Job Rate')
train.data$Permissions <- factor(train.data$Permissions)
test.data$Permissions <- factor(test.data$Permissions)
train.data$ID <- factor(train.data$ID)
test.data$ID <- factor(test.data$ID)
train.data$'Start Date' <- factor(train.data$'Start Date')
test.data$'Start Date' <- factor(test.data$'Start Date')
train.data$Education <- factor(train.data$Education)
test.data$Education <- factor(test.data$Education)
train.data$Department <- factor(train.data$Department)
test.data$Department <- factor(test.data$Department)
train.data$'Job Status' <- factor(train.data$'Job Status')
test.data$'Job Status' <- factor(test.data$'Job Status')
train.data$Location <- factor(train.data$Location)
test.data$Location <- factor(test.data$Location)
```

```{r}
# 4.Creating the tree
EmployeesTree <- rpart(Salary ~ Years + Education + Department + `Job Status` + Location + `Job Rate` + Permissions , data = train.data,parm=list(split="information"))

# 5.Visualize the decision tree using rpart.plot
rpart.plot(EmployeesTree, cex = 0.5, extra = 1)

# 6.Make predictions on the test set
predictions <- predict(EmployeesTree, test.data, type = "class")  # Assuming it's a classification problem

# 7.Evaluate the accuracy of the predictions
accuracy <- sum(predictions == test.data$Salary) / nrow(test.data)


# 8.Confusion matrix
conf_matrix <- table(predictions, test.data$Salary)

# 9.Calculate evaluation metrics
TP <- conf_matrix[2, 2]  # True Positives
TN <- conf_matrix[1, 1]  # True Negatives
FP <- conf_matrix[2, 1]  # False Positives
FN <- conf_matrix[1, 2]  # False Negatives


# 10.Calculate precision
precision <- TP / (TP + FP)

# 11.Calculate sensitivity (True Positive Rate or Recall)
sensitivity <- TP / (TP + FN)

# 12.Calculate specificity (True Negative Rate)
specificity <- TN / (TN + FP)

# 13.Output the evaluation metrics
print(paste("Accuracy:", accuracy))
print(paste("Precision:", precision))
print(paste("Sensitivity (Recall):", sensitivity))
print(paste("Specificity:",specificity))
```

##### Description of information gain tree (70% , 30%):

We can observe that we have 5 rules as we have 5 leaf nodes and 5 level,
these are rules from the above tree: 

▪ IF Job Status = 1,3 THEN it has more conditions

-   IF Job status = 1 AND Department=
    2,3,4,5,6,7,8,9,13,14,15,16,18,19,20,21 THEN Salary is 1.

-   IF Job status = 1 AND Department !=
    2,3,4,5,6,7,8,9,13,14,15,16,18,19,20,21 AND Years = 1,2,3 THEN
    Salary is 1.

-   IF Job status = 1 AND Department !=
    2,3,4,5,6,7,8,9,13,14,15,16,18,19,20,21 AND Years != 1,2,3 THEN
    Salary is 2.

-   IF Job status = 3 THEN Salary is 1.

▪ IF Job status != 1,3 THEN Salary is 3.

#### 6.1.3 information gain (75% , 25%)

```{r}
# 1.a fixed random seed to make results reproducible
set.seed(1234)
```

```{r}
# 2.Split the datasets into two subsets: Training(75%) and Testing(25%):
ind <- sample(2, nrow(Employees), replace = TRUE, prob = c(0.75, 0.25))
train.data <- Employees[ind == 1, ]
test.data <- Employees[ind == 2, ]
```

```{r}
# 3.Converting attribute to factor
train.data$Years <- factor(train.data$Years)
test.data$Years <- factor(test.data$Years)
train.data$'Job Rate' <- factor(train.data$'Job Rate')
test.data$'Job Rate' <- factor(test.data$'Job Rate')
train.data$Permissions <- factor(train.data$Permissions)
test.data$Permissions <- factor(test.data$Permissions)
train.data$ID <- factor(train.data$ID)
test.data$ID <- factor(test.data$ID)
train.data$'Start Date' <- factor(train.data$'Start Date')
test.data$'Start Date' <- factor(test.data$'Start Date')
train.data$Education <- factor(train.data$Education)
test.data$Education <- factor(test.data$Education)
train.data$Department <- factor(train.data$Department)
test.data$Department <- factor(test.data$Department)
train.data$'Job Status' <- factor(train.data$'Job Status')
test.data$'Job Status' <- factor(test.data$'Job Status')
train.data$Location <- factor(train.data$Location)
test.data$Location <- factor(test.data$Location)
```

```{r}
# 4.Creating the tree
EmployeesTree <- rpart(Salary ~ Years + Education + Department + `Job Status` + Location + `Job Rate` + Permissions , data = train.data,parm=list(split="information"))

# 5.Visualize the decision tree using rpart.plot
rpart.plot(EmployeesTree, cex = 0.5, extra = 1)

# 6.Make predictions on the test set
predictions <- predict(EmployeesTree, test.data, type = "class")  # Assuming it's a classification problem

# 7.Evaluate the accuracy of the predictions
accuracy <- sum(predictions == test.data$Salary) / nrow(test.data)



# 8.Confusion matrix
conf_matrix <- table(predictions, test.data$Salary)

# 9.Calculate evaluation metrics
TP <- conf_matrix[2, 2]  # True Positives
TN <- conf_matrix[1, 1]  # True Negatives
FP <- conf_matrix[2, 1]  # False Positives
FN <- conf_matrix[1, 2]  # False Negatives


# 10.Calculate precision
precision <- TP / (TP + FP)

# 11.Calculate sensitivity (True Positive Rate or Recall)
sensitivity <- TP / (TP + FN)

# 12.Calculate specificity (True Negative Rate)
specificity <- TN / (TN + FP)

# 13.Output the evaluation metrics
print(paste("Accuracy:", accuracy))
print(paste("Precision:", precision))
print(paste("Sensitivity (Recall):", sensitivity))
print(paste("Specificity:",specificity))
```

##### Description of information gain tree (75% , 25%):

We can observe that we have 5 rules as we have 5 leaf nodes and 5 level,
these are some rules from the above tree: 

▪ IF Job Status = 1,3 THEN it has more conditions

-   IF Job status = 1 AND Department=
    2,3,4,5,6,7,8,9,13,14,15,16,18,19,20,21 THEN Salary is 1.

-   IF Job status = 1 AND Department !=
    2,3,4,5,6,7,8,9,13,14,15,16,18,19,20,21 AND Years = 1,2,3 THEN
    Salary is 1.

-   IF Job status = 1 AND Department !=
    2,3,4,5,6,7,8,9,13,14,15,16,18,19,20,21 AND Years != 1,2,3 THEN
    Salary is 2.

-   IF Job status = 3 THEN Salary is 2.

▪ IF Job status != 1,3 THEN Salary is 3.

#### 6.1.4 information gain (65% , 35%)

```{r}
# 1.a fixed random seed to make results reproducible
set.seed(1234)
```

```{r}
# 2.Split the datasets into two subsets: Training(65%) and Testing(35%):
ind <- sample(2, nrow(Employees), replace = TRUE, prob = c(0.65, 0.35))
train.data <- Employees[ind == 1, ]
test.data <- Employees[ind == 2, ]
```

```{r}
# 3.Converting attribute to factor
train.data$Years <- factor(train.data$Years)
test.data$Years <- factor(test.data$Years)
train.data$'Job Rate' <- factor(train.data$'Job Rate')
test.data$'Job Rate' <- factor(test.data$'Job Rate')
train.data$Permissions <- factor(train.data$Permissions)
test.data$Permissions <- factor(test.data$Permissions)
train.data$ID <- factor(train.data$ID)
test.data$ID <- factor(test.data$ID)
train.data$'Start Date' <- factor(train.data$'Start Date')
test.data$'Start Date' <- factor(test.data$'Start Date')
train.data$Education <- factor(train.data$Education)
test.data$Education <- factor(test.data$Education)
train.data$Department <- factor(train.data$Department)
test.data$Department <- factor(test.data$Department)
train.data$'Job Status' <- factor(train.data$'Job Status')
test.data$'Job Status' <- factor(test.data$'Job Status')
train.data$Location <- factor(train.data$Location)
test.data$Location <- factor(test.data$Location)
```

```{r}
# 4.Creating the tree
EmployeesTree <- rpart(Salary ~ Years + Education + Department + `Job Status` + Location + `Job Rate` + Permissions , data = train.data,parm=list(split="information"))

# 5.Visualize the decision tree using rpart.plot
rpart.plot(EmployeesTree, cex = 0.5, extra = 1)

# 6.Make predictions on the test set
predictions <- predict(EmployeesTree, test.data, type = "class")  # Assuming it's a classification problem

# 7.Evaluate the accuracy of the predictions
accuracy <- sum(predictions == test.data$Salary) / nrow(test.data)

# 8.Confusion matrix
conf_matrix <- table(predictions, test.data$Salary)

# 9.Calculate evaluation metrics
TP <- conf_matrix[2, 2]  # True Positives
TN <- conf_matrix[1, 1]  # True Negatives
FP <- conf_matrix[2, 1]  # False Positives
FN <- conf_matrix[1, 2]  # False Negatives


# 10.Calculate precision
precision <- TP / (TP + FP)

# 11.Calculate sensitivity (True Positive Rate or Recall)
sensitivity <- TP / (TP + FN)

# 12.Calculate specificity (True Negative Rate)
specificity <- TN / (TN + FP)

# 13.Output the evaluation metrics
print(paste("Accuracy:", accuracy))
print(paste("Precision:", precision))
print(paste("Sensitivity (Recall):", sensitivity))
print(paste("Specificity:",specificity))
```

##### Description of information gain tree (65% , 35%):

We can observe that we have 10 rules as we have 10 leaf nodes and 8
level, these are some rules from the above tree: 

▪ IF Job Status = 1,3 THEN it has more conditions

-   IF Job status = 1 AND Permission =2,3,9,13,14 Then Salary is 1.

-   IF Job status = 1 AND Permission !=2,3,9,13,14 Department =
    2,3,4,5,6,7,8,13,14,15,16,18,19,20,21 AND Years = 0,3,6,7,8 THEN
    Salary is 1.

-   IF Job status = 1 AND Permission !=2,3,9,13,14 Department =
    2,3,4,5,6,7,8,13,14,15,16,18,19,20,21 AND Years != 0,3,6,7,8 AND
    Permission = 1,6,8,12 THEN Salary is 1.

-   IF Job status = 1 AND Permission !=2,3,9,13,14 Department =
    2,3,4,5,6,7,8,13,14,15,16,18,19,20,21 AND Years != 0,3,6,7,8 AND
    Permission != 1,6,8,12 AND Job Rate= 3,4 THEN Salary is 1.

-   IF Job status = 1 AND Permission !=2,3,9,13,14 Department =
    2,3,4,5,6,7,8,13,14,15,16,18,19,20,21 AND Years != 0,3,6,7,8 AND
    Permission != 1,6,8,12 AND Job Rate != 3,4 THEN Salary is 2.

-   IF Job status = 1 AND Permission !=2,3,9,13,14 Department !=
    2,3,4,5,6,7,8,13,14,15,16,18,19,20,21 AND Years = 1,2,3,4,6,7 AND
    Job Rate = 1,4,5 THEN Salary is 1.

-   IF Job status = 1 AND Permission !=2,3,9,13,14 Department !=
    2,3,4,5,6,7,8,13,14,15,16,18,19,20,21 AND Years = 1,2,3,4,6,7 AND
    Job Rate != 1,4,5 THEN Salary is 2.

-   IF Job status = 1 AND Permission !=2,3,9,13,14 Department !=
    2,3,4,5,6,7,8,13,14,15,16,18,19,20,21 AND Years != 1,2,3,4,6,7 THEN
    Salary is 2.

-   IF Job status != 1 THEN Salary is 2.

▪ IF Job status != 1,3 THEN Salary is 3.

#### 6.1.5 Comparison:

##### Table 2

| Information Gain | 70% Train , 30% Test | 75% Train , 25% Test | 65% Train , 35% Test |
|------------------|------------------|------------------|------------------|
| Accuracy         | 78%                  | 83%                  | 83%                  |
| Precision        | 100%                 | 92%                  | 95%                  |
| Sensitivity      | 57%                  | 67%                  | 68%                  |
| Specificity      | 100%                 | 93%                  | 96%                  |

#### 6.1.6 Discussion:

The accuracy metric measures the overall correctness of the model's
predictions. Both the 75% Train, 25% Test and 65% Train, 35% Test
partitions have higher accuracy compared to the 70% Train, 30% Test
partition.

Precision measures the proportion of correctly predicted positive
instances out of the total instances predicted as positive. The 70%
Train, 30% Test partition has the highest precision, followed by the 65%
Train, 35% Test partition, and then the 75% Train, 25% Test partition.

Sensitivity, also known as recall or true positive rate, measures the
proportion of actual positive instances correctly identified by the
model. The 65% Train, 35% Test partition has the highest sensitivity,
followed by the 75% Train, 25% Test partition, and then the 70% Train,
30% Test partition.

Specificity measures the proportion of actual negative instances
correctly identified by the model. The 65% Train, 35% Test partition has
the highest specificity, followed by the 75% Train, 25% Test partition,
and then the 70% Train, 30% Test partition.

In summary, the performance of the information gain partition varies
depending on the train-test split percentages. The 75% Train, 25% Test
and 65% Train, 35% Test partitions generally outperform the 70% Train,
30% Test partition in terms of accuracy, precision, sensitivity, and
specificity. However, the 70% Train, 30% Test partition has the highest
precision, while the 65% Train, 35% Test partition has the highest
sensitivity and specificity.

#### 6.1.7 Gini Index

#### 6.1.8 Gini Index (70% , 30%)

```{r}
# 1.a fixed random seed to make results reproducible
set.seed(1234)
```

```{r}
# 2.Split the datasets into two subsets: Training(70%) and Testing(30%):
ind <- sample(2, nrow(Employees), replace = TRUE, prob = c(0.7, 0.3))
train_data <- Employees[ind == 1, ]
test_data <- Employees[ind == 2, ]
```

```{r}
# 3.Converting attribute to factor
train_data$Years <- factor(train_data$Years)
test_data$Years <- factor(test_data$Years)
train_data$'Job Rate' <- factor(train_data$'Job Rate')
test_data$'Job Rate' <- factor(test_data$'Job Rate')
train_data$Permissions <- factor(train_data$Permissions)
test_data$Permissions <- factor(test_data$Permissions)
train_data$ID <- factor(train_data$ID)
test_data$ID <- factor(test_data$ID)
train_data$'Start Date' <- factor(train_data$'Start Date')
test_data$'Start Date' <- factor(test_data$'Start Date')
train_data$Education <- factor(train_data$Education)
test_data$Education <- factor(test_data$Education)
train_data$Department <- factor(train_data$Department)
test_data$Department <- factor(test_data$Department)
train_data$'Job Status' <- factor(train_data$'Job Status')
test_data$'Job Status' <- factor(test_data$'Job Status')
train_data$Location <- factor(train_data$Location)
test_data$Location <- factor(test_data$Location)
```

```{r}
# 4.Creating the tree
EmployeesTree <- rpart(Salary ~ Years + Education + Department + `Job Status` + Location + `Job Rate` + Permissions , data = train_data,parm=list(split="gini"))

# 5.Visualize the decision tree using rpart.plot
rpart.plot(EmployeesTree, cex = 0.5, extra = 1)

# 6.Make predictions on the test set
predictions <- predict(EmployeesTree, test_data, type = "class")  # Assuming it's a classification problem

# 7.Evaluate the accuracy of the predictions
accuracy <- sum(predictions == test_data$Salary) / nrow(test_data)

# 8.Confusion matrix
conf_matrix <- table(predictions, test_data$Salary)

# 9.Calculate evaluation metrics
TP <- conf_matrix[2, 2]  # True Positives
TN <- conf_matrix[1, 1]  # True Negatives
FP <- conf_matrix[2, 1]  # False Positives
FN <- conf_matrix[1, 2]  # False Negatives


# 10.Calculate precision
precision <- TP / (TP + FP)

# 11.Calculate sensitivity (True Positive Rate or Recall)
sensitivity <- TP / (TP + FN)

# 12.Calculate specificity (True Negative Rate)
specificity <- TN / (TN + FP)

# 13.Output the evaluation metrics
print(paste("Accuracy:", accuracy))
print(paste("Precision:", precision))
print(paste("Sensitivity (Recall):", sensitivity))
print(paste("Specificity:",specificity))
```

##### Description of Gini Index tree (70% , 30%):

We can observe that we have 5 rules as we have 5 leaf nodes and 5 level,
these are rules from the above tree: 

▪ IF Job Status = 1,3 THEN it has more conditions

-   IF Job status = 1 AND Department=
    2,3,4,5,6,7,8,9,13,14,15,16,18,19,20,21 THEN Salary is 1.

-   IF Job status = 1 AND Department !=
    2,3,4,5,6,7,8,9,13,14,15,16,18,19,20,21 AND Years = 1,2,3 THEN
    Salary is 1.

-   IF Job status = 1 AND Department !=
    2,3,4,5,6,7,8,9,13,14,15,16,18,19,20,21 AND Years != 1,2,3 THEN
    Salary is 2.

-   IF Job status = 3 THEN Salary is 2.

▪ IF Job status != 1,3 THEN Salary is 3.

#### 6.1.9 Gini Index (75% , 25%)

```{r}
# 1.a fixed random seed to make results reproducible
set.seed(1234)
```

```{r}
# 2.Split the datasets into two subsets: Training(75%) and Testing(25%):
ind <- sample(2, nrow(Employees), replace = TRUE, prob = c(0.75, 0.25))
train_data <- Employees[ind == 1, ]
test_data <- Employees[ind == 2, ]
```

```{r}
# 3.Converting attribute to factor
train_data$Years <- factor(train_data$Years)
test_data$Years <- factor(test_data$Years)
train_data$'Job Rate' <- factor(train_data$'Job Rate')
test_data$'Job Rate' <- factor(test_data$'Job Rate')
train_data$Permissions <- factor(train_data$Permissions)
test_data$Permissions <- factor(test_data$Permissions)
train_data$ID <- factor(train_data$ID)
test_data$ID <- factor(test_data$ID)
train_data$'Start Date' <- factor(train_data$'Start Date')
test_data$'Start Date' <- factor(test_data$'Start Date')
train_data$Education <- factor(train_data$Education)
test_data$Education <- factor(test_data$Education)
train_data$Department <- factor(train_data$Department)
test_data$Department <- factor(test_data$Department)
train_data$'Job Status' <- factor(train_data$'Job Status')
test_data$'Job Status' <- factor(test_data$'Job Status')
train_data$Location <- factor(train_data$Location)
test_data$Location <- factor(test_data$Location)
```

```{r}
# 4.Creating the tree
EmployeesTree <- rpart(Salary ~ Years + Education + Department + `Job Status` + Location + `Job Rate` + Permissions , data = train_data,parm=list(split="gini"))

# 5.Visualize the decision tree using rpart.plot
rpart.plot(EmployeesTree, cex = 0.5, extra = 1)

# 6.Make predictions on the test set
predictions <- predict(EmployeesTree, test_data, type = "class")  # Assuming it's a classification problem

# 7.Evaluate the accuracy of the predictions
accuracy <- sum(predictions == test_data$Salary) / nrow(test_data)

# 8.Confusion matrix
conf_matrix <- table(predictions, test_data$Salary)

# 9.Calculate evaluation metrics
TP <- conf_matrix[2, 2]  # True Positives
TN <- conf_matrix[1, 1]  # True Negatives
FP <- conf_matrix[2, 1]  # False Positives
FN <- conf_matrix[1, 2]  # False Negatives


# 10.Calculate precision
precision <- TP / (TP + FP)

# 11.Calculate sensitivity (True Positive Rate or Recall)
sensitivity <- TP / (TP + FN)

# 12.Calculate specificity (True Negative Rate)
specificity <- TN / (TN + FP)

# 13.Output the evaluation metrics
print(paste("Accuracy:", accuracy))
print(paste("Precision:", precision))
print(paste("Sensitivity (Recall):", sensitivity))
print(paste("Specificity:",specificity))
```

##### Description of Gini Index tree (75% , 25%):

We can observe that we have 5 rules as we have 5 leaf nodes and 5 level,
these are some rules from the above tree: 

▪ IF Job Status = 1,3 THEN it has more conditions

-   IF Job status = 1 AND Department=
    2,3,4,5,6,7,8,9,13,14,15,16,18,19,20,21 THEN Salary is 1.

-   IF Job status = 1 AND Department !=
    2,3,4,5,6,7,8,9,13,14,15,16,18,19,20,21 AND Years = 1,2,3 THEN
    Salary is 1.

-   IF Job status = 1 AND Department !=
    2,3,4,5,6,7,8,9,13,14,15,16,18,19,20,21 AND Years != 1,2,3 THEN
    Salary is 2.

-   IF Job status = 3 THEN Salary is 2.

▪ IF Job status != 1,3 THEN Salary is 3.

#### 6.1.10 Gini Index (65% , 35%)

```{r}
# 1.a fixed random seed to make results reproducible
set.seed(1234)
```

```{r}
# 2.Split the datasets into two subsets: Training(65%) and Testing(35%):
ind <- sample(2, nrow(Employees), replace = TRUE, prob = c(0.65, 0.35))
train_data <- Employees[ind == 1, ]
test_data <- Employees[ind == 2, ]
```

```{r}
# 3.Converting attribute to factor
train_data$Years <- factor(train_data$Years)
test_data$Years <- factor(test_data$Years)
train_data$'Job Rate' <- factor(train_data$'Job Rate')
test_data$'Job Rate' <- factor(test_data$'Job Rate')
train_data$Permissions <- factor(train_data$Permissions)
test_data$Permissions <- factor(test_data$Permissions)
train_data$ID <- factor(train_data$ID)
test_data$ID <- factor(test_data$ID)
train_data$'Start Date' <- factor(train_data$'Start Date')
test_data$'Start Date' <- factor(test_data$'Start Date')
train_data$Education <- factor(train_data$Education)
test_data$Education <- factor(test_data$Education)
train_data$Department <- factor(train_data$Department)
test_data$Department <- factor(test_data$Department)
train_data$'Job Status' <- factor(train_data$'Job Status')
test_data$'Job Status' <- factor(test_data$'Job Status')
train_data$Location <- factor(train_data$Location)
test_data$Location <- factor(test_data$Location)
```

```{r}
# 4.Creating the tree
EmployeesTree <- rpart(Salary ~ Years + Education + Department + `Job Status` + Location + `Job Rate` + Permissions , data = train_data,parm=list(split="gini"))

# 5.Visualize the decision tree using rpart.plot
rpart.plot(EmployeesTree, cex = 0.5, extra = 1)

# 6.Make predictions on the test set
predictions <- predict(EmployeesTree, test_data, type = "class")  # Assuming it's a classification problem

# 7.Evaluate the accuracy of the predictions
accuracy <- sum(predictions == test_data$Salary) / nrow(test_data)

# 8.Confusion matrix
conf_matrix <- table(predictions, test_data$Salary)

# 9.Calculate evaluation metrics
TP <- conf_matrix[2, 2]  # True Positives
TN <- conf_matrix[1, 1]  # True Negatives
FP <- conf_matrix[2, 1]  # False Positives
FN <- conf_matrix[1, 2]  # False Negatives


# 10.Calculate precision
precision <- TP / (TP + FP)

# 11.Calculate sensitivity (True Positive Rate or Recall)
sensitivity <- TP / (TP + FN)

# 12.Calculate specificity (True Negative Rate)
specificity <- TN / (TN + FP)

# 13.Output the evaluation metrics
print(paste("Accuracy:", accuracy))
print(paste("Precision:", precision))
print(paste("Sensitivity (Recall):", sensitivity))
print(paste("Specificity:",specificity))
```

##### Description of Gini Index tree (65% , 35%):

We can observe that we have 5 rules as we have 5 leaf nodes and 5 level,
these are some rules from the above tree:

▪ IF Job Status = 1,3 THEN it has more conditions

-   IF Job status = 1 AND Department=
    2,3,4,5,6,7,8,9,13,14,15,16,18,19,20,21 THEN Salary is 1.

-   IF Job status = 1 AND Department !=
    2,3,4,5,6,7,8,9,13,14,15,16,18,19,20,21 AND Years = 1,2,3 THEN
    Salary is 1.

-   IF Job status = 1 AND Department !=
    2,3,4,5,6,7,8,9,13,14,15,16,18,19,20,21 AND Years != 1,2,3 THEN
    Salary is 2.

-   IF Job status = 3 THEN Salary is 2.

▪ IF Job status != 1,3 THEN Salary is 3.

#### 6.1.11 Comparison:

##### Table 3

| Gini Index  | 70% Train , 30% Test | 75% Train , 25% Test | 65% Train , 35% Test |
|------------------|------------------|------------------|------------------|
| Accuracy    | 83%                  | 82%                  | 80%                  |
| Precision   | 92%                  | 93%                  | 100%                 |
| Sensitivity | 70%                  | 65%                  | 59%                  |
| Specificity | 93%                  | 94%                  | 100%                 |

#### 6.1.12 Discussion:

The accuracy metric measures the overall correctness of the model's
predictions. The 70% Train, 30% Test partition has the highest accuracy,
followed by the 75% Train, 25% Test partition, and then the 65% Train,
35% Test partition.

Precision measures the proportion of correctly predicted positive
instances out of the total instances predicted as positive. The 65%
Train, 35% Test partition has the highest precision, followed by the 75%
Train, 25% Test partition, and then the 70% Train, 30% Test partition.

Sensitivity, also known as recall or true positive rate, measures the
proportion of actual positive instances correctly identified by the
model. The 70% Train, 30% Test partition has the highest sensitivity,
followed by the 75% Train, 25% Test partition, and then the 65% Train,
35% Test partition.

Specificity measures the proportion of actual negative instances
correctly identified by the model. The 65% Train, 35% Test partition has
the highest specificity, followed by the 75% Train, 25% Test partition,
and then the 70% Train, 30% Test partition.

In summary, the performance of the Gini Index partition also varies
depending on the train-test split percentages. The 70% Train, 30% Test
partition generally outperforms the other partitions in terms of
accuracy. However, the 65% Train, 35% Test partition has the highest
precision, while the 70% Train, 30% Test partition has the highest
sensitivity. The 65% Train, 35% Test partition also has the highest
specificity.

#### 6.1.13 Gain Ratio

#### 6.1.14 Gain Ratio (70% , 30%)

```{r}
# 1.a fixed random seed to make results reproducible
set.seed(1234)
```

```{r}
# 2.Split the datasets into two subsets: Training(70%) and Testing(30%):
train_indices <- sample(1:nrow(Employees), nrow(Employees) * 0.7)
trainData <- Employees[train_indices, ]
testData <- Employees[-train_indices , ]

```

```{r}
# 3.Converting attribute to factor
trainData$Years <- factor(trainData$Years)
testData$Years <- factor(testData$Years)
trainData$'Job Rate' <- factor(trainData$'Job Rate')
testData$'Job Rate' <- factor(testData$'Job Rate')
trainData$Permissions <- factor(trainData$Permissions)
testData$Permissions <- factor(testData$Permissions)
trainData$ID <- factor(trainData$ID)
testData$ID <- factor(testData$ID)
trainData$'Start Date' <- factor(trainData$'Start Date')
testData$'Start Date' <- factor(testData$'Start Date')
trainData$Education <- factor(trainData$Education)
testData$Education <- factor(testData$Education)
trainData$Department <- factor(trainData$Department)
testData$Department <- factor(testData$Department)
trainData$'Job Status' <- factor(trainData$'Job Status')
testData$'Job Status' <- factor(testData$'Job Status')
trainData$Location <- factor(trainData$Location)
testData$Location <- factor(testData$Location)
trainData$Salary <- factor(trainData$Salary)
testData$Salary <- factor(testData$Salary)
```

```{r}
# 4.Creating the tree
model1 <- C5.0(Salary ~ `Job Status` + `Job Rate` + Permissions, data = trainData)

# 5.Make predictions on the test set
predictions <- predict(model1, testData, type = "class")

# 6.Evaluate the accuracy of the predictions
accuracy <- sum(predictions == testData$Salary) / nrow(testData)

# 7.Confusion matrix
conf_matrix <- table(predictions, testData$Salary)

# 8.Calculate evaluation metrics
TP <- conf_matrix[2, 2]  # True Positives
TN <- conf_matrix[1, 1]  # True Negatives
FP <- conf_matrix[2, 1]  # False Positives
FN <- conf_matrix[1, 2]  # False Negatives


# 9.Calculate precision
precision <- TP / (TP + FP)

# 10.Calculate sensitivity (True Positive Rate or Recall)
sensitivity <- TP / (TP + FN)

# 11.Calculate specificity (True Negative Rate)
specificity <- TN / (TN + FP)

# 12.Output the evaluation metrics
print(paste("Accuracy:", accuracy))
print(paste("Precision:", precision))
print(paste("Sensitivity (Recall):", sensitivity))
print(paste("Specificity:",specificity))
```

```{r}
# 13.Visualize the decision tree using plot
plot(model1)
```

##### Description of Gain Ratio tree (70% , 30%):

We can observe that we have 3 rules as we have 3 leaf nodes and 1 level,
these are some rules from the above tree: 

▪ IF Job Status = 1 THEN Salary is 1.

▪ IF Job Status = 3 THEN Salary is 2.

▪ IF Job Status = 2 THEN Salary is 3.

#### 6.1.15 Gain Ratio (75% , 25%)

```{r}
# 1.a fixed random seed to make results reproducible
set.seed(1234)
```

```{r}
# 2.Split the datasets into two subsets: Training(75%) and Testing(25%):
train_indices <- sample(1:nrow(Employees), nrow(Employees) * 0.75)
trainData <- Employees[train_indices, ]
testData <- Employees[-train_indices , ]
```

```{r}
# 3.Converting attribute to factor
trainData$Years <- factor(trainData$Years)
testData$Years <- factor(testData$Years)
trainData$'Job Rate' <- factor(trainData$'Job Rate')
testData$'Job Rate' <- factor(testData$'Job Rate')
trainData$Permissions <- factor(trainData$Permissions)
testData$Permissions <- factor(testData$Permissions)
trainData$ID <- factor(trainData$ID)
testData$ID <- factor(testData$ID)
trainData$'Start Date' <- factor(trainData$'Start Date')
testData$'Start Date' <- factor(testData$'Start Date')
trainData$Education <- factor(trainData$Education)
testData$Education <- factor(testData$Education)
trainData$Department <- factor(trainData$Department)
testData$Department <- factor(testData$Department)
trainData$'Job Status' <- factor(trainData$'Job Status')
testData$'Job Status' <- factor(testData$'Job Status')
trainData$Location <- factor(trainData$Location)
testData$Location <- factor(testData$Location)
trainData$Salary <- factor(trainData$Salary)
testData$Salary <- factor(testData$Salary)
```

```{r}
# 4.Creating the tree
model2 <- C5.0(Salary ~ `Job Status` + `Job Rate` + Department + Permissions, data = trainData, control = C5.0Control(CF = 0.1))

# 5.Make predictions on the test set
predictions <- predict(model2, testData, type = "class")

# 6.Evaluate the accuracy of the predictions
accuracy <- sum(predictions == testData$Salary) / nrow(testData)


# 7.Confusion matrix
conf_matrix <- table(predictions, testData$Salary)

# 8.Calculate evaluation metrics
TP <- conf_matrix[2, 2]  # True Positives
TN <- conf_matrix[1, 1]  # True Negatives
FP <- conf_matrix[2, 1]  # False Positives
FN <- conf_matrix[1, 2]  # False Negatives


# 9.Calculate precision
precision <- TP / (TP + FP)

# 10.Calculate sensitivity (True Positive Rate or Recall)
sensitivity <- TP / (TP + FN)

# 11.Calculate specificity (True Negative Rate)
specificity <- TN / (TN + FP)

# 12.Output the evaluation metrics
print(paste("Accuracy:", accuracy))
print(paste("Precision:", precision))
print(paste("Sensitivity (Recall):", sensitivity))
print(paste("Specificity:",specificity))
```

```{r}
# 13.Visualize the decision tree using plot
plot(model2)
```

##### Description of Gain Ratio tree (75% , 25%):

We can observe that we have 3 rules as we have 3 leaf nodes and 1 level,
these are some rules from the above tree: 

▪ IF Job Status = 1 THEN Salary is 1.

▪ IF Job Status = 3 THEN Salary is 2.

▪ IF Job Status = 2 THEN Salary is 3.

#### 6.1.16 Gain Ratio (65% , 35%)

```{r}
# 1.a fixed random seed to make results reproducible
set.seed(1234)
```

```{r}
# 2.Split the datasets into two subsets: Training(65%) and Testing(35%):
train_indices <- sample(1:nrow(Employees), nrow(Employees) * 0.65)
trainData <- Employees[train_indices, ]
testData <- Employees[-train_indices , ]
```

```{r}
# 3.Converting attribute to factor
trainData$Years <- factor(trainData$Years)
testData$Years <- factor(testData$Years)
trainData$'Job Rate' <- factor(trainData$'Job Rate')
testData$'Job Rate' <- factor(testData$'Job Rate')
trainData$Permissions <- factor(trainData$Permissions)
testData$Permissions <- factor(testData$Permissions)
trainData$ID <- factor(trainData$ID)
testData$ID <- factor(testData$ID)
trainData$'Start Date' <- factor(trainData$'Start Date')
testData$'Start Date' <- factor(testData$'Start Date')
trainData$Education <- factor(trainData$Education)
testData$Education <- factor(testData$Education)
trainData$Department <- factor(trainData$Department)
testData$Department <- factor(testData$Department)
trainData$'Job Status' <- factor(trainData$'Job Status')
testData$'Job Status' <- factor(testData$'Job Status')
trainData$Location <- factor(trainData$Location)
testData$Location <- factor(testData$Location)
trainData$Salary <- factor(trainData$Salary)
testData$Salary <- factor(testData$Salary)
```

```{r}
# 4.Train a CART model
model3 <- C5.0(Salary ~ `Job Status` + `Job Rate` + Department + Permissions, data = trainData, control = C5.0Control(CF = 0.1))


# 5.Make predictions on the test dataset
predictions <- predict(model3, testData, type = "class")


# 6.Evaluate the accuracy of the predictions
accuracy <- sum(predictions == testData$Salary) / nrow(testData)


# 7.Confusion matrix
conf_matrix <- table(predictions, testData$Salary)

# 8.Calculate evaluation metrics
TP <- conf_matrix[2, 2]  # True Positives
TN <- conf_matrix[1, 1]  # True Negatives
FP <- conf_matrix[2, 1]  # False Positives
FN <- conf_matrix[1, 2]  # False Negatives


# 9.Calculate precision
precision <- TP / (TP + FP)

# 10.Calculate sensitivity (True Positive Rate or Recall)
sensitivity <- TP / (TP + FN)

# 11.Calculate specificity (True Negative Rate)
specificity <- TN / (TN + FP)

# 12.Output the evaluation metrics
print(paste("Accuracy:", accuracy))
print(paste("Precision:", precision))
print(paste("Sensitivity (Recall):", sensitivity))
print(paste("Specificity:",specificity))

```

```{r}
# 13.Visualize the decision tree using plot
plot(model3)
```

##### Description of Gain Ratio tree (65% , 35%):

We can observe that we have 3 rules as we have 3 leaf nodes and 1 level,
these are some rules from the above tree: 

▪ IF Job Status = 1 THEN Salary is 1.

▪ IF Job Status = 3 THEN Salary is 2.

▪ IF Job Status = 2 THEN Salary is 3.

#### 6.1.17 Comparison:

##### Table 4

| Gain Ratio  | 70% Train , 30% Test | 75% Train , 25% Test | 65% Train , 35% Test |
|------------------|------------------|------------------|------------------|
| Accuracy    | 86%                  | 86%                  | 85%                  |
| Precision   | 100%                 | 1%                   | 100%                 |
| Sensitivity | 60%                  | 60%                  | 61%                  |
| Specificity | 100%                 | 100%                 | 100%                 |

#### 6.1.18 Discussion:

The accuracy metric measures the overall correctness of the model's
predictions. The 70% Train, 30% Test and 75% Train, 25% Test partitions
have the highest accuracy, followed by the 65% Train, 35% Test
partition.

Precision measures the proportion of correctly predicted positive
instances out of the total instances predicted as positive. The 70%
Train, 30% Test and 65% Train, 35% Test partitions have the highest
precision, while the 75% Train, 25% Test partition has a very low
precision.

Sensitivity, also known as recall or true positive rate, measures the
proportion of actual positive instances correctly identified by the
model. The sensitivity values are similar across all three partitions,
with the 65% Train, 35% Test partition having a slightly higher
sensitivity.

Specificity measures the proportion of actual negative instances
correctly identified by the model. All three partitions have a perfect
specificity, indicating that they correctly identify all negative
instances.

In summary, the Gain Ratio partition shows relatively consistent
performance across different train-test split percentages. The 70%
Train, 30% Test and 75% Train, 25% Test partitions have similar
accuracy, sensitivity, and specificity values. However, the 75% Train,
25% Test partition has a significantly lower precision compared to the
other partitions. The 70% Train, 30% Test and 65% Train, 35% Test
partitions have the highest precision and similar sensitivity and
specificity values.

### 5.2.0 Clustring

We used Clustering techniques to apply unsupervised learning to our
data. Clustering groups similar data pointstogether based on some
characteristics.The primary goal of clustering is to find natural
groupings or clusters within our dataset without any prior knowledge of
the groups. In this section we are going to partition our data using
k-means. we are going to try three different k-means values which are
(2,3 and 4).For each one we will calculate the average silhouette, total
within-cluster sum of square and the BCubed precision and recall.

### 5.2.1 Silhouette coefficient

The average silhouette coefficient used to evaluate the quality of
clusters formed by a clustering algorithm, such as K-means or
hierarchical clustering. It gives an overall evaluation of the
suitability of the clustering and quantifies how well each data point
fits into the designated cluster.

### 5.2.2 BCubed precision and recall

BCubed is used to assess the quality of clustering results, It focuses
on assessing the precision and recall of clustered data points
concerning their true categories or classes. Precision of an object
indicates how many other objects in the same cluster belong to the same
category as the object. Recall of an object reflects how many objects of
the same category are assigned to the same cluster.

### 5.2.3 Total within-cluster sum of square

Total within-cluster sum of square measures the compactness or
homogeneity of clusters by calculating the sum of squared distances
between each data point and the centroid of its assigned cluster.

### 5.2.4 Elbow method

(factoextra) pakage making easy to extract and visualize the output
of exploratory multivariate data analyses. ^[6]^

the elbow approach is a technique used to find the ideal number of
clusters in a dataset. It aids in determining the point at which the
number of clusters increases and the quality of clustering improves less
and less.

#### 6.2.0 Clustring

#### 6.2.1 First K-mean (k=2)

1.  Deleting columns

```{r}
Sal <- Employees$Salary
##Delete columns
Employees <- Employees[, !names(Employees) %in% c("Salary","ID", "Start Date")]
```

Reason for removing some columns: We removed "Salary" because it is our
class label and we are dealing with an unsupervised learning technique
which means we don't need the class label. We removed " Start Date"
because it had "unknown" type and it was not possible to convert its
type to numric in order to start with clustering. We as well removed
"ID" because it is of type "character" and converting it to numric will
help us with nothing in our dataset.

2.  Creating two clusters \# Cluster k=2

```{r}
#calculate k-mean k=2
km <- kmeans(Employees, centers= 2, iter.max = 100 , algorithm="Lloyd", nstart=100)
km
```

3.  visualizing the result in graphical format of two clusters

```{r}
#plot k-mean 
fviz_cluster(list(data = Employees, cluster = km$cluster),
             ellipse.type = "norm", geom = "point", stand = FALSE,
             palette = "jco", ggtheme = theme_classic())
```

Description of plotting the k-mean: visualizing results of a k-means
clustering in a graphical format.

4.  calculating the Average Silhouette Coefficient of two clusters

```{r}
#avg silhouette
library(cluster)
silh <- silhouette(km$cluster, dist(Employees))
rownames(silh) <- rownames(Employees)
fviz_silhouette(silh)
```

5.  calculating the Total Within Cluster Sum of two clusters

```{r}
# Total sum of squares
km$tot.withinss
```

6.  calculating BCubed precision and recall of two clusters

```{r}
cluster_assignments <- c(km$cluster) 
ground_truth_labels <- c(Sal)  
data <- data.frame(cluster = cluster_assignments, label = ground_truth_labels) 
# Function to calculate BCubed precision and recall
calculate_bcubed_metrics <- function(data) {
  n <- nrow(data)
  precision_sum <- 0
  recall_sum <- 0
  
  for (i in 1:n) {
    cluster <- data$cluster[i]
    label <- data$label[i]
    
    # Count the number of items from the same category within the same cluster
    same_category_same_cluster <- sum(data$label[data$cluster == cluster] == label)
    
    # Count the total number of items in the same cluster
    total_same_cluster <- sum(data$cluster == cluster)
    
    # Count the total number of items with the same category
    total_same_category <- sum(data$label == label)
    
    # Calculate precision and recall for the current item and add them to the sums
    precision_sum <- precision_sum + same_category_same_cluster / total_same_cluster
    recall_sum <- recall_sum + same_category_same_cluster / total_same_category
  }
  
  # Calculate average precision and recall
  precision <- precision_sum / n
  recall <- recall_sum / n
  
  return(list(precision = precision, recall = recall))
}

# Calculate BCubed precision and recall
metrics <- calculate_bcubed_metrics(data)  
# Extract precision and recall from the metrics
precision <- metrics$precision
recall <- metrics$recall 
# Print the results 
cat("BCubed Precision:", precision, "\n") 
cat("BCubed Recall:", recall, "\n")
```

#### 6.2.2 Second K-mean (k=3)

1.  creating the 3 clusters

#### Cluster k=3

```{r}
#calculate k-mean k=3
km <- kmeans(Employees, centers= 3, iter.max = 100 , algorithm="Lloyd", nstart=100)
km
```

2.  visualizing the result in graphical format of three clusters

```{r}
#plot k-mean 
fviz_cluster(list(data = Employees, cluster = km$cluster),
             ellipse.type = "norm", geom = "point", stand = FALSE,
             palette = "jco", ggtheme = theme_classic())
```

3.  calculating the Average Silhouette Coefficient of three clusters

```{r}
#avg silhouette
library(cluster)
silh <- silhouette(km$cluster, dist(Employees))
rownames(silh) <- rownames(Employees)
fviz_silhouette(silh)
```

4.  calculating the Total Within Cluster Sum of three clusters

```{r}
# Total sum of squares
km$tot.withinss
```

5.  calculating BCubed precision and recall of three clusters

```{r}
cluster_assignments <- c(km$cluster) 
ground_truth_labels <- c(Sal)  
data <- data.frame(cluster = cluster_assignments, label = ground_truth_labels) 
# Function to calculate BCubed precision and recall
calculate_bcubed_metrics <- function(data) {
  n <- nrow(data)
  precision_sum <- 0
  recall_sum <- 0
  
  for (i in 1:n) {
    cluster <- data$cluster[i]
    label <- data$label[i]
    
    # Count the number of items from the same category within the same cluster
    same_category_same_cluster <- sum(data$label[data$cluster == cluster] == label)
    
    # Count the total number of items in the same cluster
    total_same_cluster <- sum(data$cluster == cluster)
    
    # Count the total number of items with the same category
    total_same_category <- sum(data$label == label)
    
    # Calculate precision and recall for the current item and add them to the sums
    precision_sum <- precision_sum + same_category_same_cluster / total_same_cluster
    recall_sum <- recall_sum + same_category_same_cluster / total_same_category
  }
  
  # Calculate average precision and recall
  precision <- precision_sum / n
  recall <- recall_sum / n
  
  return(list(precision = precision, recall = recall))
}

# Calculate BCubed precision and recall
metrics <- calculate_bcubed_metrics(data)  
# Extract precision and recall from the metrics
precision <- metrics$precision
recall <- metrics$recall 
# Print the results 
cat("BCubed Precision:", precision, "\n") 
cat("BCubed Recall:", recall, "\n")
```

#### 6.2.3 Third K-mean (k=4)

1.  creating the 4 clusters

#### Cluster k=4

```{r}
#calculate k-mean k=4
km <- kmeans(Employees, centers= 4, iter.max = 100 , algorithm="Lloyd", nstart=100)
km
```

2.visualizing the result in graphical format of four clusters

```{r}
#plot k-mean 
fviz_cluster(list(data = Employees, cluster = km$cluster),
             ellipse.type = "norm", geom = "point", stand = FALSE,
             palette = "jco", ggtheme = theme_classic())
```

3.  calculating the Average Silhouette Coefficient of four clusters

```{r}
#avg silhouette
library(cluster)
silh <- silhouette(km$cluster, dist(Employees))
rownames(silh) <- rownames(Employees)
fviz_silhouette(silh)
```

4.  calculating the Total Within Cluster Sum of four clusters

```{r}
# Total sum of squares
km$tot.withinss
```

5.  calculating BCubed precision and recall of four clusters

```{r}
cluster_assignments <- c(km$cluster) 
ground_truth_labels <- c(Sal)  
data <- data.frame(cluster = cluster_assignments, label = ground_truth_labels) 
# Function to calculate BCubed precision and recall
calculate_bcubed_metrics <- function(data) {
  n <- nrow(data)
  precision_sum <- 0
  recall_sum <- 0
  
  for (i in 1:n) {
    cluster <- data$cluster[i]
    label <- data$label[i]
    
    # Count the number of items from the same category within the same cluster
    same_category_same_cluster <- sum(data$label[data$cluster == cluster] == label)
    
    # Count the total number of items in the same cluster
    total_same_cluster <- sum(data$cluster == cluster)
    
    # Count the total number of items with the same category
    total_same_category <- sum(data$label == label)
    
    # Calculate precision and recall for the current item and add them to the sums
    precision_sum <- precision_sum + same_category_same_cluster / total_same_cluster
    recall_sum <- recall_sum + same_category_same_cluster / total_same_category
  }
  
  # Calculate average precision and recall
  precision <- precision_sum / n
  recall <- recall_sum / n
  
  return(list(precision = precision, recall = recall))
}

# Calculate BCubed precision and recall
metrics <- calculate_bcubed_metrics(data)  
# Extract precision and recall from the metrics
precision <- metrics$precision
recall <- metrics$recall 
# Print the results 
cat("BCubed Precision:", precision, "\n") 
cat("BCubed Recall:", recall, "\n")
```

#### 6.2.4 Using Elbow method

```{r}
##### Elbow with wss 
fviz_nbclust(Employees, kmeans, method = "wss")+ labs(subtitle = "Elbow method")
```

#### 6.2.5 Disscution

In this Elbow method graph, The highest point signifies the optimal
number of clusters in the dataset. and here the highest point is 2 which
means the number of clusters is set to 2. (elbow method does not always
produce a clear, distinct "highest" point).

#### 6.2.6 Comparison:

##### Table 5

|                                    | k=2                       | k=3                       | k=4                       |
|--------------------|-----------------|-----------------|-----------------|
| Average Silhoutte width            | 0.3                      | 0.25                      | 0.22                      |
| total within-cluster sum of square | 40750.1                   | 34529.21                  | 29822.08                  |
| BCubed precision                   | 0.3349891                 | 0.3352973                 | 0.3362805                 |
| BCubed recall                      | 0.512721                 | 0.3487142                 | 0.2555154                 |
| Visualization                      | all figure is shown above | all figure is shown above | all figure is shown above |

#### 6.2.7 Discussion:

From the results of the methods implemented on "Employees" data set we
obtained the following: the model with the number of clusters k=2 had
average silhouette width=0.3 it is the best result since the model with
the number of clusters k=3 and 4 had average silhouette width=0.25 and
0.22 respectively this means that the the model with the number of
clusters k=2 has the highest average silhouette result, this value
indicates that that objects within the same cluster are close to each
other and as far as possible to the objects in the other cluster.

For total within cluster sum results of model with the number of
clusters k=2 we had a result with the value of 40750.1 this value
represents the distances between each data point in a cluster and the
centroid of that cluster, it is which is considered relatively large
comparing with the other two models' results, for model with the number
of clusters k=3 and 4 we had the results 34529.21 and 29822.08
respectively, in my opinion as the number of clusters (k) increases,
the total within cluster sum generally decreases because the data points
are closer to their cluster centroids.

BCubed precision and recall results showed that model with the number of
clusters k=2 has the highest precision with the value 0.3349891 and the
highest recall with the value 0.512721, the model with the number of
clusters k=3 and 4 had precision values of 0.3352973 and 0.3362805
respectively and the model with the number of clusters k=3 snd 4 had
recall values of 0.3487142 and 0.2555154 respectively, A higher
precision indicates that, within a cluster, the data points are more
homogeneous and accurately assigned to the same group and higher recall
implies that the clustering algorithm is effective in identifying and
grouping together all instances of a specific category.

The model with the number of clusters k=2 seems to be the optimal number
of clustering since it had the highest average silhouette width, and the highest value of precision and recall,
this means that most of the evaluation methods' results supports it,
we also applied the Elbow Method and we had the result of k=2.

### **7.0 Findings:**

In the beginning, we selected a dataset that represents the Sala'ry data
to predict the probability of their Salary in a new jobs. To have
efficient, correct, and the best possible accuracy results we applied
several preprocessing techniques that improve the efficiency of the
data. We applied several plotting methods such as boxplot, histogram and
bar chart to illustrate the data so it can help us understand our data
and make the appropriate preprocessing techniques. Based on the blots
and other commands, we removed all null, missing, and outliers' values
that can badly affect the results. Also, we applied data transformation,
so we normalized discretized, and incdeed some attributes to give them
equal weight and to facilitate handling the data during data mining
tasks. Consequently, we applied the data mining tasks which are
classification and clustering.

#### Classification

For classification, we use the decision tree method to construct our
model, we tried 3 different sizes of training and testing data to get
the best result for construction and evaluation and different methods.
So we concluded the following results: Information Gain:

|             | 70% training set 30% testing set: | 70% training set 30% testing set: | 70% training set 30% testing set: | 75% training set 25% testing set: | 75% training set 25% testing set: | 75% training set 25% testing set: | 65% training set 35% testing set: | 65% training set 35% testing set: | 65% training set 35% testing set: |
|--------|:------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|
|             |                IG                 |             IG ratio              |            Gini Index             |                IG                 |             IG ratio              |            Gini Index             |                IG                 |             IG ratio              |            Gini Index             |
| Accuracy    |                78%                |                86%                |                83%                |                83%                |                86%                |                82%                |                83%                |                85%                |                80%                |
| Precision   |               100%                |               100%                |                92%                |                92%                |                1%                 |                93%                |                95%                |               100%                |               100%                |
| Sensitivity |                57%                |                60%                |                70%                |                67%                |                60%                |                65%                |                68%                |                61%                |                59%                |
| Specificity |               100%                |               100%                |                93%                |                93%                |               100%                |                94%                |                96%                |               100%                |               100%                |

#### Partition 1 (70% training set, 30% testing set):

IG algorithm: The accuracy is 78%, indicating that the model correctly
predicted 78% of the instances in the testing set. The precision is
100%, meaning that all the instances predicted as positive by the model
were actually positive. The sensitivity is 57%, indicating that the
model correctly identified 57% of the positive instances. The
specificity is 100%, meaning that the model correctly identified all the
negative instances.

IG Ratio algorithm: The accuracy is 86%, indicating that the model
correctly predicted 86% of the instances in the testing set. The
precision is 100%, meaning that all the instances predicted as positive
by the model were actually positive. The sensitivity is 60%, indicating
that the model correctly identified 60% of the positive instances. The
specificity is 100%, meaning that the model correctly identified all the
negative instances.

Gini Index algorithm: The accuracy is 83%, indicating that the model
correctly predicted 83% of the instances in the testing set. The
precision is 92%, meaning that 92% of the instances predicted as
positive by the model were actually positive. The sensitivity is 70%,
indicating that the model correctly identified 70% of the positive
instances. The specificity is 93%, meaning that the model correctly
identified 93% of the negative instances.

When we choose the best algorithm, we go towards the algorithm with the
highest number of attributes and the highest accuracy. Therefore, we
will exclude the IG Ratio algorithm because it uses 4 attributes, which
is less than the number of attributes in the rest of the algorithms. The
Gini Index algorithm achieved an accuracy of 83%, which is the highest
among the remaining algorithms in this partition. It had a higher
accuracy compared to the IG algorithm (78%).

#### Partition 2 (75% training set, 25% testing set):

IG algorithm: The accuracy is 83%, indicating that the model correctly
predicted 83% of the instances in the testing set. The precision is 92%,
meaning that 92% of the instances predicted as positive by the model
were actually positive. The sensitivity is 67%, indicating that the
model correctly identified 67% of the positive instances. The
specificity is 93%, meaning that the model correctly identified 93% of
the negative instances.

IG Ratio algorithm: The accuracy is 86%, indicating that the model
correctly predicted 86% of the instances in the testing set. The
precision is 1%, meaning that only 1% of the instances predicted as
positive by the model were actually positive. The sensitivity is 60%,
indicating that the model correctly identified 60% of the positive
instances. The specificity is 100%, meaning that the model correctly
identified all the negative instances.

Gini Index algorithm: The accuracy is 82%, indicating that the model
correctly predicted 82% of the instances in the testing set. The
precision is 93%, meaning that 93% of the instances predicted as
positive by the model were actually positive. The sensitivity is 65%,
indicating that the model correctly identified 65% of the positive
instances. The specificity is 94%, meaning that the model correctly
identified 94% of the negative instances.

When we choose the best algorithm, we go towards the algorithm with the
highest number of attributes and the highest accuracy. Therefore, we
will exclude the IG Ratio algorithm because it uses 4 attributes, which
is less than the number of attributes in the rest of the algorithms. The
Gini Index algorithm achieved an accuracy of 82%, which is the highest
among the remaining algorithms in this partition. It outperformed the IG
algorithm (83%) in terms of accuracy.

#### Partition 3 (65% training set, 35% testing set):

IG algorithm: The accuracy is 83%, indicating that the model correctly
predicted 83% of the instances in the testing set. The precision is 95%,
meaning that 95% of the instances predicted as positive by the model
were actually positive. The sensitivity is 68%, indicating that the
model correctly identified 68% of the positive instances. The
specificity is 96%, meaning that the model correctly identified 96% of
the negative instances.

IG Ratio algorithm: The accuracy is 85%, indicating that the model
correctly predicted 85% of the instances in the testing set. The
precision is 100%, meaning that all the instances predicted as positive
by the model were actually positive. The sensitivity is 61%, indicating
that the model correctly identified 61% of the positive instances. The
specificity is 100%, meaning that the model correctly identified all the
negative instances.

Gini Index algorithm: The accuracy is 80%, indicating that the model
correctly predicted 80% of the instances in the testing set. The
precision is 100%, meaning that all the instances predicted as positive
by the model were actually positive. The sensitivity is 59%, indicating
that the model correctly identified 59% of the positive instances. The
specificity is 100%, meaning that the model correctly identified all the
negative instances.

When we choose the best algorithm, we go towards the algorithm with the
highest number of attributes and the highest accuracy. Therefore, we
will exclude the IG Ratio algorithm because it uses 4 attributes, which
is less than the number of attributes in the rest of the algorithms. The
IG algorithm achieved an accuracy of 83%, which is the highest among the
remaining algorithms in this partition. It had a higher accuracy
compared to the Gini Index algorithm (80%).

**Overall Best Algorithm (based on accuracy):**

The Gini Index algorithm achieved the highest accuracy across all
partitions. While the accuracy of the Gini Index algorithm varied
slightly in each partition (83% in Partition 1, 82% in Partition 2, and
80% in Partition 3), it consistently outperformed the other algorithms
in terms of accuracy.

**Best Decision Tree:**

After careful review, we concluded that the Information Gain algorithm
-65% training set and 35% testing set- produced the most accurate and
dependable findings. As a result, we confidently chose it as the best
model to build a decision tree. 

The model was selected for our decision tree due to its higher accuracy,
precision, and specificity compared to other algorithms and sizes.
Furthermore, it covers an extensive assessment of many attributes,
taking into account their value via feature selection algorithms. This
technique allows us to use a big of relevant attributes, which improves
the decision tree's overall performance. Moreover, this model
effectively collects the most informative features that significantly
contribute to the target variable, resulting in specific conditions for
decision making.

**Several findings can be drawn from the decision tree analysis:**

1.  The decision tree has 10 leaf nodes, indicating that the tree has
    extracted 10 rules or distinct conditions.

2.  The decision nodes in the tree apply the attributes that have a
    significant impact to make decisions and classify cases.

3.  The tree indicates that Job Rate, Job Status, Years, Department, and
    Permission will determine the good and fair Salary for the Arab
    Region Employees.

Consequently, the Information Gain algorithm emerged as the best
selection, providing decision trees with high accuracy, precision,
specificity, and more specific conditions based on feature selection.

#### Clustering 

In order to determine the ideal number of clusters for clustering, we
employed the K-means algorithm with three distinct K. We then computed
the average silhouette width for each K, and came to the following
conclusions: When cluster (K) is equal to 2, the average silhouette
width is 0.29. When cluster (K) is equal to 3, the average silhouette
width is 0.24.When cluster (K) is equal to 4, the average silhouette
width is 0.21. • The average silhouette width is 0.246. Since the 2-Mean
model has the best average silhouette width---meaning that items within
the same cluster are as close to each other as possible and as far away
from the objects in the other cluster as possible---it has the ideal
number of clusters. Additionally, as the figures in the assessment and
comparison section demonstrate, when compared to the other models, the
2-Mean clustering has the least amount of cluster overlapping.

Finally, both models are helpful for predicting Salaries and helped us
to reach our goal which is helping people to take in consideration if
they having a good salary at their current job or the new job also!, but
Since our data contains a class label "Salary" This makes Supervised
Learning models(classification) more accurate and suitable to apply than
unsupervised learning model(clustering), as the expected output is known
beforehand this way we makes use of the class label attribute.

### 8.0 Reference

[1] QUSAY AL-BTOUSH, Employees, Kaggle, 2021,Available:
<https://www.kaggle.com/datasets/qusaybtoush1990/employes.> Accessed on:
September 10, 2023

[2] [1] S.-A. N. Alexandropoulos, S. B. Kotsiantis, and M. N. Vrahatis,
"Data preprocessing in predictive data mining," *The Knowledge
Engineering Review*, vol. 34, no. e1, pp. 1-33, 2019. DOI:
10.1017/S026988891800036X.

[3] RDocumentation, "Information Gain - RDocumentation," RDocumentation,
Available:
<https://www.rdocumentation.org/packages/FSelectorRcpp/versions/0.3.11/topics/information_gain,>
Accessed on: November 8, 2023.

[4] RDocumentation, "rpart - RDocumentation," RDocumentation, Available:
<https://www.rdocumentation.org/packages/rpart/versions/4.1.21/topics/rpart>[,](https://www.rdocumentation.org/packages/FSelectorRcpp/versions/0.3.11/topics/information_gain,)
Accessed on: November 10, 2023.

[5] RDocumentation, "C5.0 - RDocumentation," RDocumentation, Available:
\<<https://www.rdocumentation.org/packages/C50/versions/0.1.8/topics/C5.0.default>,)
Accessed on: November 10, 2023.

[6] G. K. Roussel, M. Chessel, and S. J. Champely, "Factoextra: Extract
and Visualize the Results of Multivariate Data Analyses," R package
version 1.0.7, Extract and Visualize the Results of Multivariate Data
Analyses, 2021. [Online]. Available:
<https://cran.r-project.org/package=factoextra>. Accessed: November 29,
2023.
